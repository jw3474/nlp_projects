{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original train set: 60000\n",
      "size of original test set: 20000\n",
      "****************************************************************************************************\n",
      "size of train set: 60000, #positive: 30055, #negative: 29945\n",
      "size of test set: 1000, #positive: 510, #negative: 490\n",
      "['it', 'will', 'help', 'relieve', 'your', 'stress', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken']\n",
      "sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tweets_data/vocabulary.pkl\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('tweets_data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('tweets_data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "print(\"size of original train set: {}\".format(len(train_tweets)))\n",
    "print(\"size of original test set: {}\".format(len(test_tweets)))\n",
    "\n",
    "# only select first 1000 test sample for test\n",
    "test_tweets = test_tweets[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"size of train set: {}, #positive: {}, #negative: {}\".format(len(train_tweets), np.sum(train_labels), len(train_tweets)-np.sum(train_labels)))\n",
    "print(\"size of test set: {}, #positive: {}, #negative: {}\".format(len(test_tweets), np.sum(test_labels), len(test_tweets)-np.sum(test_labels)))\n",
    "\n",
    "# show text of the idx-th train tweet\n",
    "# The 'padtoken' is used to ensure each tweet has the same length\n",
    "idx = 100\n",
    "train_text = [vocabulary[x] for x in train_tweets[idx]]\n",
    "print(train_text)\n",
    "sentiment_label = [\"negative\", \"positive\"]\n",
    "print(\"sentiment: {}\".format(sentiment_label[train_labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a single-layer lstm network\n",
    "\n",
    "First of all, we'll build a single-layer LSTM network for the analysis.\n",
    "\n",
    "1. Train the network for 1000 iterations. In each iteration, use batch_size samples to train the network.\n",
    "2. For every 50 iterations, apply the network on the test set, and print out the test accuracy and mean loss.\n",
    "\n",
    "With these settings, what is the accuracy and then parameter tuning to achieve a higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a linear layer, y = x*w + b\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        init = tf.truncated_normal([shape[-1], output_size], mean=0.0, stddev=1.0 / shape[-1]**0.5)\n",
    "        W = tf.get_variable(\"weight\", initializer=init)\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_onehot = tf.one_hot(tweets, depth=vocab_size, axis=-1)\n",
    "\n",
    "# define the lstm cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "#init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "#outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, dtype=tf.float32)\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "sentiment = linear(final_state[-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(test_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss = 0.6926367878913879, test loss = 0.6922867426872253, train acc = 0.53125, test acc = 0.547\n",
      "step 50: train loss = 0.5944596529006958, test loss = 0.646370602607727, train acc = 0.6875, test acc = 0.6290000009536744\n",
      "step 100: train loss = 0.6180109977722168, test loss = 0.5917654404640198, train acc = 0.671875, test acc = 0.676\n",
      "step 150: train loss = 0.5727746486663818, test loss = 0.5866021471023559, train acc = 0.734375, test acc = 0.6969999995231628\n",
      "step 200: train loss = 0.5294105410575867, test loss = 0.5623033666610717, train acc = 0.703125, test acc = 0.7170000004768372\n",
      "step 250: train loss = 0.45725739002227783, test loss = 0.5448545160293579, train acc = 0.765625, test acc = 0.7210000004768372\n",
      "step 300: train loss = 0.6063907146453857, test loss = 0.5411413726806641, train acc = 0.71875, test acc = 0.7269999990463257\n",
      "step 350: train loss = 0.5294427871704102, test loss = 0.5587043895721435, train acc = 0.75, test acc = 0.7090000009536743\n",
      "step 400: train loss = 0.548490583896637, test loss = 0.5367403512001038, train acc = 0.75, test acc = 0.7389999990463256\n",
      "step 450: train loss = 0.6172195672988892, test loss = 0.5284256753921509, train acc = 0.703125, test acc = 0.7459999990463256\n",
      "step 500: train loss = 0.5506560802459717, test loss = 0.5231889352798462, train acc = 0.71875, test acc = 0.745\n",
      "step 550: train loss = 0.5481210947036743, test loss = 0.5210894293785095, train acc = 0.71875, test acc = 0.7410000009536744\n",
      "step 600: train loss = 0.4210825562477112, test loss = 0.5437724232673645, train acc = 0.828125, test acc = 0.7140000009536743\n",
      "step 650: train loss = 0.5312641859054565, test loss = 0.5231827425956727, train acc = 0.78125, test acc = 0.749\n",
      "step 700: train loss = 0.45852917432785034, test loss = 0.5103711366653443, train acc = 0.765625, test acc = 0.7619999990463256\n",
      "step 750: train loss = 0.5956020951271057, test loss = 0.5164627757072449, train acc = 0.75, test acc = 0.7509999990463256\n",
      "step 800: train loss = 0.588720440864563, test loss = 0.515058503627777, train acc = 0.71875, test acc = 0.7479999990463256\n",
      "step 850: train loss = 0.4793497920036316, test loss = 0.5194300093650818, train acc = 0.828125, test acc = 0.7490000009536744\n",
      "step 900: train loss = 0.5159651637077332, test loss = 0.5106004476547241, train acc = 0.8125, test acc = 0.7499999995231629\n",
      "step 950: train loss = 0.4747043251991272, test loss = 0.5071971130371093, train acc = 0.8125, test acc = 0.7540000004768371\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "num_steps = 1000\n",
    "start_id = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        #print(step)\n",
    "        batch_tweets = train_tweets[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "        batch_labels = train_labels[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "        if start_id + batch_size >= train_tweets.shape[0]:\n",
    "            start_id = 0\n",
    "        else:\n",
    "            start_id += batch_size\n",
    "        feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "        _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        if (step % 50 == 0):\n",
    "            num_itrs = int(np.ceil(test_tweets.shape[0] / batch_size))\n",
    "            test_loss = 0\n",
    "            test_acc = 0\n",
    "            for i in range(num_itrs):\n",
    "                #print(i)\n",
    "                test_start_id = i * batch_size\n",
    "                test_end_id = min((i+1) * batch_size, test_tweets.shape[0])\n",
    "                batch_tweets = test_tweets[test_start_id:test_end_id]\n",
    "                batch_labels = test_labels[test_start_id:test_end_id]\n",
    "                feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "                test_l, test_a = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "                test_loss += test_l * (test_end_id - test_start_id)\n",
    "                test_acc += test_a * (test_end_id - test_start_id)\n",
    "            test_loss /= test_tweets.shape[0]\n",
    "            test_acc /= test_tweets.shape[0]\n",
    "            print(\"step {}: train loss = {}, test loss = {}, train acc = {}, test acc = {}\".format(step, train_loss, test_loss, train_acc, test_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a two-layer lstm network\n",
    "\n",
    "Next, we look at a slightly more difficult network structure: a double-layer LSTM. The output of the first LSTM cell is propagated to the second LSTM cell. We only need to make small modifications to the previous network to construct this one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_onehot = tf.one_hot(tweets, depth=vocab_size, axis=-1)\n",
    "\n",
    "# define the lstm cell\n",
    "double_lstm_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(hidden_size) for _ in range(2)])\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "#init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "#outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(double_lstm_cell, tweets_onehot, dtype=tf.float32)\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "sentiment = linear(final_state[-1][-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss = 0.6932448148727417, test loss = 0.6926844396591186, train acc = 0.4375, test acc = 0.539\n",
      "step 50: train loss = 0.5435563325881958, test loss = 0.6287727279663086, train acc = 0.734375, test acc = 0.6510000009536743\n",
      "step 100: train loss = 0.585548996925354, test loss = 0.590353440284729, train acc = 0.671875, test acc = 0.6829999990463257\n",
      "step 150: train loss = 0.5590313673019409, test loss = 0.5840771899223328, train acc = 0.75, test acc = 0.7080000009536743\n",
      "step 200: train loss = 0.5114988088607788, test loss = 0.5538660912513733, train acc = 0.765625, test acc = 0.7199999990463257\n",
      "step 250: train loss = 0.45484456419944763, test loss = 0.5377914710044861, train acc = 0.75, test acc = 0.735\n",
      "step 300: train loss = 0.5865883231163025, test loss = 0.5355859475135804, train acc = 0.71875, test acc = 0.7410000004768371\n",
      "step 350: train loss = 0.5432904958724976, test loss = 0.5504963159561157, train acc = 0.734375, test acc = 0.727\n",
      "step 400: train loss = 0.5681610107421875, test loss = 0.5248734230995178, train acc = 0.734375, test acc = 0.7539999990463256\n",
      "step 450: train loss = 0.5797514915466309, test loss = 0.5353078541755676, train acc = 0.703125, test acc = 0.7400000009536744\n",
      "step 500: train loss = 0.5533156394958496, test loss = 0.5182287673950196, train acc = 0.734375, test acc = 0.745\n",
      "step 550: train loss = 0.5688567757606506, test loss = 0.5146489233970643, train acc = 0.6875, test acc = 0.7549999990463256\n",
      "step 600: train loss = 0.4233821630477905, test loss = 0.5231055631637573, train acc = 0.828125, test acc = 0.737\n",
      "step 650: train loss = 0.5504534244537354, test loss = 0.5168154335021973, train acc = 0.75, test acc = 0.756\n",
      "step 700: train loss = 0.463176429271698, test loss = 0.5080539832115173, train acc = 0.765625, test acc = 0.7699999990463257\n",
      "step 750: train loss = 0.5928098559379578, test loss = 0.5079823040962219, train acc = 0.75, test acc = 0.755\n",
      "step 800: train loss = 0.5936626195907593, test loss = 0.5101420173645019, train acc = 0.71875, test acc = 0.75\n",
      "step 850: train loss = 0.4744349718093872, test loss = 0.5160601410865784, train acc = 0.8125, test acc = 0.7470000009536744\n",
      "step 900: train loss = 0.5197031497955322, test loss = 0.5105808844566345, train acc = 0.78125, test acc = 0.7450000004768371\n",
      "step 950: train loss = 0.45480456948280334, test loss = 0.5056350607872009, train acc = 0.84375, test acc = 0.7519999990463256\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "num_steps = 1000\n",
    "start_id = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        #print(step)\n",
    "        batch_tweets = train_tweets[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "        batch_labels = train_labels[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "        if start_id + batch_size >= train_tweets.shape[0]:\n",
    "            start_id = 0\n",
    "        else:\n",
    "            start_id += batch_size\n",
    "        feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "        _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        if (step % 50 == 0):\n",
    "            num_itrs = int(np.ceil(test_tweets.shape[0] / batch_size))\n",
    "            test_loss = 0\n",
    "            test_acc = 0\n",
    "            for i in range(num_itrs):\n",
    "                #print(i)\n",
    "                test_start_id = i * batch_size\n",
    "                test_end_id = min((i+1) * batch_size, test_tweets.shape[0])\n",
    "                batch_tweets = test_tweets[test_start_id:test_end_id]\n",
    "                batch_labels = test_labels[test_start_id:test_end_id]\n",
    "                feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "                test_l, test_a = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "                test_loss += test_l * (test_end_id - test_start_id)\n",
    "                test_acc += test_a * (test_end_id - test_start_id)\n",
    "            test_loss /= test_tweets.shape[0]\n",
    "            test_acc /= test_tweets.shape[0]\n",
    "            print(\"step {}: train loss = {}, test loss = {}, train acc = {}, test acc = {}\".format(step, train_loss, test_loss, train_acc, test_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Lookup layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define an embedding layer\n",
    "\n",
    "It's not hard to imagine in the previous practices, the input we fed in are very sparse because each word was represented as a one-hot vector. This makes it difficult for the network to understand what story the input data is telling. \n",
    "\n",
    "Word embedding: instead of using a one-hot vector to represent each word, we can add an word embedding matrix in which each word is represented as a low-dimensional vector. Note that this representation is not sparse any more, because we're working in a continuous vector space now. Words that share similar/related semantic meaning should be 'close to each other' in this vector space (we could define a distance measure to estimate the closeness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding(input_, vocab_size, output_size, name):\n",
    "    \"\"\"\n",
    "    1. Define an embedding matrix\n",
    "    2. return both the lookup results and the embedding matrix.\n",
    "    \"\"\"\n",
    "    word_embed = tf.get_variable(name=name, shape=[vocab_size, output_size])\n",
    "    lookup = tf.nn.embedding_lookup(word_embed, input_)\n",
    "    return (lookup, word_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a single lstm network with embedding layer\n",
    "\n",
    "Build a single-layer LSTM network according to the network structure. Then, train the network with the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EMBEDDING SINGLE-LAYER LSTM\n",
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_embed, embed = embedding(tweets, vocab_size, embed_size, 'embed')\n",
    "\n",
    "# define the lstm cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "#init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "#outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_embed, dtype=tf.float32)\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "sentiment = linear(final_state[-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss = 0.6931838989257812, test loss = 0.6927768788337707, train acc = 0.46875, test acc = 0.5430000004768372\n",
      "step 50: train loss = 0.5806010961532593, test loss = 0.613920443058014, train acc = 0.7109375, test acc = 0.6589999995231628\n",
      "step 100: train loss = 0.5398669242858887, test loss = 0.5551366386413574, train acc = 0.75, test acc = 0.7169999995231628\n",
      "step 150: train loss = 0.6041603684425354, test loss = 0.544174467086792, train acc = 0.7109375, test acc = 0.7410000023841858\n",
      "step 200: train loss = 0.5747165679931641, test loss = 0.5248050346374512, train acc = 0.71875, test acc = 0.760000002861023\n",
      "step 250: train loss = 0.5489137768745422, test loss = 0.518502375125885, train acc = 0.703125, test acc = 0.752\n",
      "step 300: train loss = 0.42131972312927246, test loss = 0.5149349765777588, train acc = 0.828125, test acc = 0.760000002861023\n",
      "step 350: train loss = 0.45854446291923523, test loss = 0.5054869620800019, train acc = 0.8046875, test acc = 0.7700000019073486\n",
      "step 400: train loss = 0.5302026271820068, test loss = 0.5080785751342773, train acc = 0.7265625, test acc = 0.7589999990463256\n",
      "step 450: train loss = 0.5194768905639648, test loss = 0.503450492143631, train acc = 0.796875, test acc = 0.7630000019073486\n",
      "step 500: train loss = 0.44703924655914307, test loss = 0.5061266126632691, train acc = 0.7734375, test acc = 0.7580000004768371\n",
      "step 550: train loss = 0.46269187331199646, test loss = 0.5011374881267547, train acc = 0.765625, test acc = 0.7599999980926514\n",
      "step 600: train loss = 0.45657169818878174, test loss = 0.4979318091869354, train acc = 0.765625, test acc = 0.7640000014305115\n",
      "step 650: train loss = 0.4706166982650757, test loss = 0.5064416489601136, train acc = 0.765625, test acc = 0.7660000019073486\n",
      "step 700: train loss = 0.5255182981491089, test loss = 0.5091097569465637, train acc = 0.75, test acc = 0.7649999990463257\n",
      "step 750: train loss = 0.4622397720813751, test loss = 0.5247864785194397, train acc = 0.78125, test acc = 0.7660000014305115\n",
      "step 800: train loss = 0.43447962403297424, test loss = 0.5013474013805389, train acc = 0.8046875, test acc = 0.768000002861023\n",
      "step 850: train loss = 0.5244350433349609, test loss = 0.522426169872284, train acc = 0.75, test acc = 0.7479999990463256\n",
      "step 900: train loss = 0.5022369623184204, test loss = 0.5169827089309692, train acc = 0.8046875, test acc = 0.7560000014305115\n",
      "step 950: train loss = 0.450303316116333, test loss = 0.5046981060504914, train acc = 0.84375, test acc = 0.7709999995231629\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "num_steps = 1000\n",
    "start_id = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(num_steps):\n",
    "    #print(step)\n",
    "    batch_tweets = train_tweets[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "    batch_labels = train_labels[start_id:min(start_id + batch_size, train_tweets.shape[0])]\n",
    "    if start_id + batch_size >= train_tweets.shape[0]:\n",
    "        start_id = 0\n",
    "    else:\n",
    "        start_id += batch_size\n",
    "    feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "    _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    if (step % 50 == 0):\n",
    "        num_itrs = int(np.ceil(test_tweets.shape[0] / batch_size))\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        for i in range(num_itrs):\n",
    "            #print(i)\n",
    "            test_start_id = i * batch_size\n",
    "            test_end_id = min((i+1) * batch_size, test_tweets.shape[0])\n",
    "            batch_tweets = test_tweets[test_start_id:test_end_id]\n",
    "            batch_labels = test_labels[test_start_id:test_end_id]\n",
    "            feed_dict = {tweets: batch_tweets, labels: batch_labels}\n",
    "            test_l, test_a = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "            test_loss += test_l * (test_end_id - test_start_id)\n",
    "            test_acc += test_a * (test_end_id - test_start_id)\n",
    "        test_loss /= test_tweets.shape[0]\n",
    "        test_acc /= test_tweets.shape[0]\n",
    "        print(\"step {}: train loss = {}, test loss = {}, train acc = {}, test acc = {}\".format(step, train_loss, test_loss, train_acc, test_acc))  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word vectors via tSNE\n",
    "\n",
    "First, we retrieve **embedding matrix** from the network. Then use tSNE to reduce each low-dimensional word vector into a 2D vector. \n",
    "\n",
    "And then, we visualize some interesting word pairs in 2D panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_male = [\"men\", \"women\", \"king\", \"queen\"]\n",
    "country_capital = [\"spain\", \"madrid\", \"italy\", \"rome\", \"japan\", \"tokyo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# get the value of our embedding matrix\n",
    "embed_mat = sess.run(embed)\n",
    "dim2_embed = TSNE(n_components=2).fit_transform(embed_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7597, 2)\n"
     ]
    }
   ],
   "source": [
    "reverse_vocab = {}\n",
    "for k in range(vocab_size):\n",
    "    v = vocabulary[k]\n",
    "    reverse_vocab[v] = k\n",
    "print(dim2_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cc27ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_words = female_male\n",
    "#plot_words = country_capital\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "for i in range(len(plot_words)):\n",
    "    wordvec = dim2_embed[reverse_vocab[plot_words[i].lower()]]\n",
    "    plot_x.append(wordvec[0])\n",
    "    plot_y.append(wordvec[1])\n",
    "plt.figure()\n",
    "plt.scatter(plot_x, plot_y)\n",
    "for i in range(len(plot_words)):\n",
    "    w = plot_words[i]\n",
    "    plt.annotate(w, xy=(plot_x[i], plot_y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11da86d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_words = country_capital\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "for i in range(len(plot_words)):\n",
    "    wordvec = dim2_embed[reverse_vocab[plot_words[i].lower()]]\n",
    "    plot_x.append(wordvec[0])\n",
    "    plot_y.append(wordvec[1])\n",
    "plt.figure()\n",
    "plt.scatter(plot_x, plot_y)\n",
    "for i in range(len(plot_words)):\n",
    "    w = plot_words[i]\n",
    "    plt.annotate(w, xy=(plot_x[i], plot_y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
